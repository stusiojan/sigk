# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vw_PVgb2pnI5EV5atQOYjIvgInZC7qBA
"""

# @title 1. Instalacja pakietów
!pip install lpips torchmetrics scikit-image opencv-python-headless tqdm matplotlib
!pip install flip-evaluator

# @title 2. Przygotowanie Środowiska

import os
from datetime import datetime
import matplotlib.pyplot as plt
import json
import zipfile
import torch
from google.colab import drive

# ===============================================
# 1. Montaż dysku
# ===============================================
if not os.path.exists('/content/drive'):
    drive.mount('/content/drive')


# ===============================================
# 2. Rozpakowanie datasetu
# ===============================================
ZIP_PATH = '/content/drive/MyDrive/studia/SIGK/PROJ_3/dataset.zip'
EXTRACT_PATH = '/content/project_data'

if not os.path.exists(EXTRACT_PATH):
    print("Rozpakowywanie zbioru danych...")
    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
        zip_ref.extractall(EXTRACT_PATH)
    print("Gotowe.")
else:
    print("Dane już są rozpakowane.")

# ===============================================
# 3. Przygotowanie folderu na wyniki
# ===============================================
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
EXPERIMENT_NAME = f"NeuralRender_{timestamp}"
EXPERIMENT_DIR = f"/content/drive/MyDrive/studia/SIGK/PROJ_3/wyniki_{EXPERIMENT_NAME}"

os.makedirs(EXPERIMENT_DIR, exist_ok=True)
print(f"Wyniki będą zapisywane w: {EXPERIMENT_DIR}")

# @title 3. Definicja Datasetu

import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from PIL import Image
import torchvision.transforms as T

class PhongDataset(Dataset):
    def __init__(self, root_dir, csv_file, transform=None):
        self.root_dir = root_dir
        self.data = pd.read_csv(os.path.join(root_dir, csv_file))
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]

        img_path = os.path.join(self.root_dir, row['filename'])
        image = Image.open(img_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        camera_pos_raw = torch.tensor([5.0, 5.0, 15.0])# Stała pozycja kamery
        obj_pos_raw = torch.tensor([row['obj_x'], row['obj_y'], row['obj_z']])
        dist = torch.norm(camera_pos_raw - obj_pos_raw)

        # INVERSE DEPTH (1 / odległosć od kamery)
        inv_dist = 1.0 / (dist + 1e-6)

        # normalizowane parametry
        obj_pos = obj_pos_raw / 4.0
        light_pos = torch.tensor([row['light_x'], row['light_y'], row['light_z']]) / 20.0
        diffuse = torch.tensor([row['diff_r'], row['diff_g'], row['diff_b']]) / 255.0
        shininess = torch.tensor([row['shininess']]) / 20.0
        rel_light = light_pos - obj_pos


        params = torch.cat([
            obj_pos,      # 3
            diffuse,      # 3
            shininess,    # 1
            light_pos,    # 3
            rel_light,    # 3
            inv_dist.unsqueeze(0) # 1
        ], dim=0).float()

        return params, image

def save_sample_image(gen_model, dataset, epoch, save_path):
  """
  Funkcja pomocnicza do zapisywania próbek wizualnych w trakcie treningu
  """
  gen_model.eval()
  with torch.no_grad():
      params, real_img = dataset[0]
      params = params.unsqueeze(0).to(device)

      fake_img = gen_model(params).detach().cpu().squeeze(0)

      fake_img = (fake_img + 1) / 2
      real_img = (real_img + 1) / 2

      fig, ax = plt.subplots(1, 2, figsize=(8, 4))
      ax[0].imshow(real_img.permute(1, 2, 0).clip(0, 1))
      ax[0].set_title("Ground Truth")
      ax[0].axis('off')

      ax[1].imshow(fake_img.permute(1, 2, 0).clip(0, 1))
      ax[1].set_title(f"Epoch {epoch}")
      ax[1].axis('off')

      plt.savefig(f"{save_path}/epoch_{epoch:03d}.png")
      plt.close()
  gen_model.train()

# @title 4. Definicja Dataloadera

transforms = T.Compose([
    T.Resize((128, 128)),
    T.ToTensor(),
    T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
])

dataset_path = os.path.join(EXTRACT_PATH, 'dataset')
dataset = PhongDataset(dataset_path, 'metadata.csv', transform=transforms)

# Train/Test (80%/20%)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])

train_loader = DataLoader(train_set, batch_size=64, shuffle=True)
test_loader = DataLoader(test_set, batch_size=64, shuffle=False)

print(f"Dane gotowe. Trening: {len(train_set)}, Test: {len(test_set)}")

# @title 5. Architektura cGAN
import torch.nn as nn

class Generator(nn.Module):
    def __init__(self, input_dim=14):
        super().__init__()

        self.fc = nn.Sequential(
            nn.Linear(input_dim, 512 * 4 * 4),
            nn.BatchNorm1d(512 * 4 * 4),
            nn.ReLU(True)
        )

        self.conv = nn.Sequential(
            # ZMIANA 1: Zmieniamy 512 na 514 (512 cech + 2 kanały współrzędnych)
            nn.ConvTranspose2d(514, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            # Reszta pozostaje bez zmian...
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(True),
            nn.ConvTranspose2d(32, 3, 4, 2, 1, bias=False),
            nn.Tanh()
        )

    def forward(self, x):
        x = self.fc(x)
        x = x.view(-1, 512, 4, 4)

        # ZMIANA 2: Tworzenie i doklejanie kanałów CoordConv
        batch_size, _, h, w = x.shape

        # Tworzenie siatki X i Y (znormalizowanej od -1 do 1)
        xx_channel = torch.linspace(-1, 1, w, device=x.device).view(1, 1, 1, w).expand(batch_size, 1, h, w)
        yy_channel = torch.linspace(-1, 1, h, device=x.device).view(1, 1, h, 1).expand(batch_size, 1, h, w)

        # Doklejenie do tensora wejściowego (concat w wymiarze kanałów)
        x = torch.cat([x, xx_channel, yy_channel], dim=1) # Teraz kształt to (B, 514, 4, 4)

        return self.conv(x)


class Discriminator(nn.Module):
    def __init__(self, input_dim=14):
        super().__init__()
        self.img_layer = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1), nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2),
        ) # Output: 16x16

        self.param_layer = nn.Sequential(
            nn.Linear(input_dim, 16*16), nn.LeakyReLU(0.2)
        )

        self.final = nn.Sequential(
            nn.Conv2d(256+1, 512, 4, 2, 1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2),
            nn.Conv2d(512, 1, 8, 1, 0), nn.Sigmoid()
        )

    def forward(self, img, params):
        img_feat = self.img_layer(img)
        param_feat = self.param_layer(params).view(-1, 1, 16, 16)
        combined = torch.cat([img_feat, param_feat], dim=1)
        return self.final(combined).view(-1, 1).squeeze(1)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
netG = Generator().to(device)
netD = Discriminator().to(device)
print("Modele zainicjalizowane.")

# @title 6. Trening (POPRAWIONY)
import torch.optim as optim
from tqdm.notebook import tqdm
import torch
import torch.nn as nn
import time
import lpips
import json

# ===============================================
# 1. Konfiguracja treningu
# ===============================================
optG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))
optD = optim.Adam(netD.parameters(), lr=0.0001, betas=(0.5, 0.999))

schedulerG = torch.optim.lr_scheduler.StepLR(optG, step_size=50, gamma=0.1)
schedulerD = torch.optim.lr_scheduler.StepLR(optD, step_size=50, gamma=0.1)

criterion_GAN = nn.BCELoss()
# criterion_L1 nie jest tu potrzebne jako obiekt, liczymy je ręcznie
criterion_lpips = lpips.LPIPS(net='alex').to(device)

EPOCHS = 100
WARMUP_EPOCHS = 10
L1_LAMBDA = 450.0   # Wysoka waga dla kolorów

history = {'G_loss': [], 'D_loss': []}

print(f"Start treningu: {EPOCHS} epok (Warmup: {WARMUP_EPOCHS})")
print("-" * 60)

# ===============================================
# 2. Trening
# ===============================================
for epoch in range(EPOCHS):
    epoch_start = time.time()
    is_warmup = epoch < WARMUP_EPOCHS

    running_g_loss = 0.0
    running_d_loss = 0.0

    desc_text = f"Epoka {epoch+1}/{EPOCHS}" + (" [WARMUP]" if is_warmup else "")
    loop = tqdm(train_loader, leave=False, desc=desc_text)

    for i, (params, real_img) in enumerate(loop):
        params = params.to(device)
        real_img = real_img.to(device)
        batch_size = real_img.size(0)

        # -----------------------------------------------
        # 1. Generowanie obrazu (potrzebne dla D i G)
        # -----------------------------------------------
        fake_img = netG(params)

        # ===============================================
        # 2.1. Trenowanie Dyskryminatora
        # ===============================================
        netD.zero_grad()
        loss_D = torch.tensor(0.0, device=device)

        if not is_warmup:
            label_real = torch.full((batch_size,), 0.9, device=device)
            label_fake = torch.zeros(batch_size, device=device)

            output_real = netD(real_img, params)
            loss_D_real = criterion_GAN(output_real, label_real)

            # Detach, aby nie psuć gradientów generatora w tym kroku
            output_fake = netD(fake_img.detach(), params)
            loss_D_fake = criterion_GAN(output_fake, label_fake)

            loss_D = (loss_D_real + loss_D_fake) * 0.5
            loss_D.backward()
            optD.step()

        # ===============================================
        # 2.2. Trenowanie Generatora
        # ===============================================
        netG.zero_grad()

        # --- A. Obliczenia wspólne (Masked L1 + LPIPS) ---

        # 1. Masked L1 Loss
        # Maska: Gdzie jest obiekt? (zakładamy tło ~ -1, więc próg -0.95 jest bezpieczny)
        object_mask = (real_img.mean(dim=1, keepdim=True) > -0.95).float()

        abs_diff = torch.abs(fake_img - real_img)
        loss_obj = (abs_diff * object_mask).mean()       # Błąd na obiekcie
        loss_bg = (abs_diff * (1 - object_mask)).mean()  # Błąd tła

        # Ważona suma L1 (obiekt 100x ważniejszy)
        loss_G_L1 = (loss_obj * 100.0 + loss_bg * 1.0) * L1_LAMBDA

        # 2. LPIPS Loss (Perceptual)
        loss_G_perceptual = criterion_lpips(fake_img, real_img).mean()

        # --- B. Obliczenia specyficzne dla fazy ---

        if not is_warmup:
            # Tryb normalny: Dodajemy oszustwo Dyskryminatora
            # Ponowne przejście przez D (tym razem z gradientami dla G)
            output_fake_G = netD(fake_img, params)
            label_real_for_G = torch.ones(batch_size, device=device)

            loss_G_GAN = criterion_GAN(output_fake_G, label_real_for_G)

            # Suma wszystkich strat
            loss_G = loss_G_GAN + loss_G_L1 + (loss_G_perceptual * 10.0)
        else:
            # Tryb Warmup: Tylko rekonstrukcja (L1 + LPIPS), bez GAN
            loss_G = loss_G_L1 + (loss_G_perceptual * 10.0)

        loss_G.backward()
        optG.step()

        running_d_loss += loss_D.item()
        running_g_loss += loss_G.item()
        loop.set_postfix(D=loss_D.item(), G=loss_G.item())

    # ===============================================
    # 2.3 KONIEC EPOKI
    # ===============================================
    schedulerG.step()
    schedulerD.step()

    avg_d = running_d_loss / len(train_loader)
    avg_g = running_g_loss / len(train_loader)

    history['G_loss'].append(avg_g)
    history['D_loss'].append(avg_d)

    status = "[WARMUP]" if is_warmup else ""
    print(f"Epoka [{epoch+1}/{EPOCHS}] {status} | Loss D: {avg_d:.4f} | Loss G: {avg_g:.4f}")

    save_sample_image(netG, test_set, epoch+1, EXPERIMENT_DIR)

    # Zapis co 10 epok LUB w ostatniej epoce
    if (epoch + 1) % 10 == 0 or (epoch + 1) == EPOCHS:
        torch.save(netG.state_dict(), f"{EXPERIMENT_DIR}/generator_epoch_{epoch+1}.pth")

    with open(f"{EXPERIMENT_DIR}/loss_history.json", 'w') as f:
        json.dump(history, f)

print("-" * 60)
print("Trening zakończony.")

# @title 4. Obliczanie Metryk
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
from scipy.spatial.distance import directed_hausdorff
import cv2
from tqdm.notebook import tqdm
import lpips
from torchmetrics.image import StructuralSimilarityIndexMeasure
from flip_evaluator import evaluate as flip_eval

# ===============================================
# 0. Funkcje pomocnicze
# ===============================================

def tensor_to_numpy_for_flip(tensor):
    """
    tensor PyTorch (C, H, W) [-1, 1] -> numpy array (H, W, C) [0, 1].
    """
    img_np = (tensor.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0
    return np.clip(img_np, 0, 1)

def calculate_hausdorff_metric(img_tensor_real, img_tensor_fake):
    real = (img_tensor_real.permute(1, 2, 0).cpu().numpy() * 127.5 + 127.5).astype(np.uint8)
    fake = (img_tensor_fake.permute(1, 2, 0).cpu().numpy() * 127.5 + 127.5).astype(np.uint8)

    edges_real = cv2.Canny(real, 100, 200)
    edges_fake = cv2.Canny(fake, 100, 200)

    pts_real = np.argwhere(edges_real > 0)
    pts_fake = np.argwhere(edges_fake > 0)

    if len(pts_real) == 0 or len(pts_fake) == 0: return 0.0
    d1 = directed_hausdorff(pts_real, pts_fake)[0]
    d2 = directed_hausdorff(pts_fake, pts_real)[0]
    return max(d1, d2)

# ===============================================
# 1. Obliczanie metryk
# ===============================================
netG.eval()
results = {'LPIPS': [], 'SSIM': [], 'ALIP': [], 'Hausdorff': []}
print("Obliczanie metryk na całym zbiorze testowym (LPIPS, SSIM, ALIP, Hausdorff)...")

loss_lpips = lpips.LPIPS(net='alex').to(device)

metric_ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)

with torch.no_grad():
    for params, real_img in tqdm(test_loader, leave=False):
        params, real_img = params.to(device), real_img.to(device)
        fake_img = netG(params)

        # ===============================================
        # 1.1. LPIPS i SSIM w batchach na gpu
        # ===============================================
        r_01 = (real_img + 1) / 2.0
        f_01 = (fake_img + 1) / 2.0

        r_01 = torch.clamp(r_01, 0, 1)
        f_01 = torch.clamp(f_01, 0, 1)

        results['LPIPS'].extend(loss_lpips(f_01, r_01).cpu().numpy().flatten())
        results['SSIM'].append(metric_ssim(f_01, r_01).item())

        # ===============================================
        # 1.2. FLIP & Hausdorff pojedyńczo na cpu
        # ===============================================
        for i in range(len(real_img)):
            results['Hausdorff'].append(calculate_hausdorff_metric(real_img[i], fake_img[i]))

            ref_np = tensor_to_numpy_for_flip(real_img[i])
            test_np = tensor_to_numpy_for_flip(fake_img[i])

            # (error_map, mean_error, parameters)
            _, mean_error, _ = flip_eval(ref_np, test_np, "LDR")
            results['ALIP'].append(mean_error)

avg_lpips = np.mean(results['LPIPS'])
avg_ssim = np.mean(results['SSIM'])
avg_alip = np.mean(results['ALIP'])
avg_haus = np.mean(results['Hausdorff'])

# ===============================================
# 3. Wizualizacja
# ===============================================

indices_to_show = [0, 10, 20]
if len(test_set) < 21: indices_to_show = [0]

fig, axs = plt.subplots(len(indices_to_show), 4, figsize=(16, 4 * len(indices_to_show)))
plt.subplots_adjust(wspace=0.1, hspace=0.25)

if len(indices_to_show) == 1: axs = np.expand_dims(axs, axis=0)

for i, idx in enumerate(indices_to_show):
    params, real_img_tensor = test_set[idx]
    params = params.unsqueeze(0).to(device)

    with torch.no_grad():
        fake_img_tensor = netG(params).squeeze(0)

    ref_np = tensor_to_numpy_for_flip(real_img_tensor)
    test_np = tensor_to_numpy_for_flip(fake_img_tensor)
    flip_map, flip_score, _ = flip_eval(ref_np, test_np, "LDR", applyMagma=True)

    l1_diff = np.abs(ref_np - test_np).mean(axis=2)

    # Kolumna 1
    axs[i, 0].imshow(ref_np)
    axs[i, 0].set_title("Referencja")
    axs[i, 0].axis('off')

    # Kolumna 2
    axs[i, 1].imshow(test_np)
    axs[i, 1].set_title(f"Ours Neural Render")
    axs[i, 1].axis('off')

    # Kolumna 3
    im1 = axs[i, 2].imshow(l1_diff, cmap='gray', vmin=0, vmax=0.2)
    axs[i, 2].set_title("Difference (L1)")
    axs[i, 2].axis('off')

    # Kolumna 4
    axs[i, 3].imshow(flip_map)
    axs[i, 3].set_title(f"FLIP Map (Error: {flip_score:.4f})")
    axs[i, 3].axis('off')

plt.savefig(f"{EXPERIMENT_DIR}/wizualizacja_koncowa.png", dpi=150, bbox_inches='tight')
plt.show()

# ===============================================
# 4. Raport
# ===============================================
report = f"""
=== RAPORT KOŃCOWY ===
Eksperyment: {EXPERIMENT_NAME}
Data: {datetime.now().strftime("%Y-%m-%d %H:%M")}
----------------------
METRYKI JAKOŚCI (zbiór testowy, N={len(test_set)}):

1. FLIP       : {avg_alip:.6f}

2. LPIPS      : {avg_lpips:.6f}

3. SSIM       : {avg_ssim:.6f}

4. Hausdorff  : {avg_haus:.4f} px

Wizualizacja zapisana w:
{EXPERIMENT_DIR}/wizualizacja_koncowa.png
"""
print(report)
with open(f"{EXPERIMENT_DIR}/wyniki.txt", "w") as f:
    f.write(report)